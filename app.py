import streamlit as st
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Nh·∫≠n d·∫°ng c·∫£m x√∫c khu√¥n m·∫∑t",
    page_icon="üòä",
    layout="wide"
)

# C√°c l·ªõp c·∫£m x√∫c
EMOTION_CLASSES = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']
EMOTION_COLORS = {
    'Angry': '#FF4444',
    'Disgust': '#8B4513',
    'Fear': '#800080',
    'Happy': '#32CD32',
    'Neutral': '#808080',
    'Sad': '#4169E1',
    'Surprise': '#FF8C00'
}


# Cache model ƒë·ªÉ tr√°nh load l·∫°i nhi·ªÅu l·∫ßn
@st.cache_resource
def load_model():
    """Load model ƒë√£ train s·∫µn"""
    try:
        model = tf.keras.models.load_model('fer_cnn.h5')
        return model
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ load model: {e}")
        st.info("Vui l√≤ng ƒë·∫£m b·∫£o file model t·ªìn t·∫°i v√† ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c")
        return None


def preprocess_image(image):
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh cho model"""
    # Chuy·ªÉn sang grayscale n·∫øu c·∫ßn
    if len(image.shape) == 3:
        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    # Resize v·ªÅ k√≠ch th∆∞·ªõc model expect (th∆∞·ªùng l√† 48x48 cho emotion recognition)
    image = cv2.resize(image, (48, 38))

    # Normalize pixel values
    image = image.astype('float32') / 255.0

    # Expand dimensions ƒë·ªÉ ph√π h·ª£p v·ªõi input model
    image = np.expand_dims(image, axis=0)
    image = np.expand_dims(image, axis=-1)

    return image


def detect_faces(image):
    """Ph√°t hi·ªán khu√¥n m·∫∑t trong ·∫£nh"""
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    return faces


# def predict_emotion(model, face_image):
#     """D·ª± ƒëo√°n c·∫£m x√∫c t·ª´ ·∫£nh khu√¥n m·∫∑t"""
#     processed_image = preprocess_image(face_image)
#     prediction = model.predict(processed_image, verbose=0)[0]
#     emotion_index = np.argmax(prediction)
#     confidence = np.max(prediction) * 100
#     return EMOTION_CLASSES[emotion_index], confidence

def predict_top_emotions(model, face_image, top_k=3):
    processed_image = preprocess_image(face_image)
    prediction = model.predict(processed_image, verbose=0)[0]
    top_indices = prediction.argsort()[-top_k:][::-1]
    top_emotions = [(EMOTION_CLASSES[i], float(prediction[i]) * 100) for i in top_indices]
    return top_emotions


class VideoTransformer(VideoTransformerBase):
    """Class x·ª≠ l√Ω video stream t·ª´ camera"""

    def __init__(self):
        self.model = load_model()

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.model is not None:
            faces = detect_faces(img_rgb)

            for (x, y, w, h) in faces:
                # V·∫Ω khung quanh khu√¥n m·∫∑t
                cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)

                # C·∫Øt v√πng khu√¥n m·∫∑t
                face = img_rgb[y:y + h, x:x + w]

                if face.size > 0:
                    try:
                        top_emotions = predict_top_emotions(self.model, face, top_k=3)

                        for i, (emo, conf) in enumerate(top_emotions):
                            cv2.putText(img, f"{emo}: {conf:.1f}%",
                                        (x, y - 10 - i * 20),
                                        cv2.FONT_HERSHEY_SIMPLEX,
                                        0.5, (0, 255, 0), 1)

                    except Exception as e:
                        cv2.putText(img, "Error", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,
                                    0.6, (0, 0, 255), 2)

        return img


def main():
    st.title("üé≠ ·ª®ng d·ª•ng nh·∫≠n d·∫°ng c·∫£m x√∫c khu√¥n m·∫∑t")
    st.markdown("---")

    # Load model
    model = load_model()

    if model is None:
        st.stop()

    # Sidebar cho c√°c t√πy ch·ªçn
    st.sidebar.title("T√πy ch·ªçn")
    option = st.sidebar.selectbox(
        "Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠n d·∫°ng:",
        ["üì§ Upload ·∫£nh", "üì∑ S·ª≠ d·ª•ng Camera"]
    )

    if option == "üì§ Upload ·∫£nh":
        st.header("Upload ·∫£nh ƒë·ªÉ nh·∫≠n d·∫°ng c·∫£m x√∫c")

        uploaded_file = st.file_uploader(
            "Ch·ªçn ·∫£nh...",
            type=['png', 'jpg', 'jpeg'],
            help="H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: PNG, JPG, JPEG"
        )

        if uploaded_file is not None:
            # Hi·ªÉn th·ªã ·∫£nh ƒë√£ upload
            image = Image.open(uploaded_file)
            image_array = np.array(image)

            col1, col2 = st.columns(2)

            with col1:
                st.subheader("·∫¢nh g·ªëc")
                st.image(image, use_container_width=True)

            with col2:
                st.subheader("K·∫øt qu·∫£ nh·∫≠n d·∫°ng")

                faces = detect_faces(image_array)

                if len(faces) > 0:
                    result_image = image_array.copy()
                    emotions_detected = []

                    for i, (x, y, w, h) in enumerate(faces):
                        cv2.rectangle(result_image, (x, y), (x + w, y + h), (255, 0, 0), 2)

                        face = image_array[y:y + h, x:x + w]

                        top_emotions = predict_top_emotions(model, face, top_k=3)
                        emotions_detected.append(top_emotions)  # l∆∞u c·∫£ top 3

                        best_emotion, best_conf = top_emotions[0]
                        cv2.putText(result_image, f"{best_emotion}: {best_conf:.1f}%",
                                    (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 5)

                    st.image(result_image, use_container_width=True)

                    st.subheader("Chi ti·∫øt k·∫øt qu·∫£:")
                    for i, top_emotions in enumerate(emotions_detected):
                        st.markdown(f"<strong>Khu√¥n m·∫∑t {i + 1}:</strong>", unsafe_allow_html=True)
                        for emo, conf in top_emotions:
                            color = EMOTION_COLORS.get(emo, '#000000')
                            st.markdown(f"""
                            <div style='padding: 5px; border-left: 4px solid {color}; margin: 3px 0;'>
                                {emo}: {conf:.1f}%
                            </div>
                            """, unsafe_allow_html=True)
                else:
                    st.warning("Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t trong ·∫£nh!")
                    st.info("üí° M·∫πo: H√£y th·ª≠ v·ªõi ·∫£nh c√≥ khu√¥n m·∫∑t r√µ r√†ng v√† kh√¥ng b·ªã che khu·∫•t")

    elif option == "üì∑ S·ª≠ d·ª•ng Camera":
        st.header("S·ª≠ d·ª•ng Camera ƒë·ªÉ nh·∫≠n d·∫°ng c·∫£m x√∫c real-time")

        st.info("üìå Cho ph√©p truy c·∫≠p camera khi ƒë∆∞·ª£c y√™u c·∫ßu")

        # C·∫•u h√¨nh WebRTC
        rtc_configuration = RTCConfiguration({
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        })

        webrtc_ctx = webrtc_streamer(
            key="emotion-detection",
            video_transformer_factory=VideoTransformer,
            rtc_configuration=rtc_configuration,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

        st.markdown("### üìä Th·ªëng k√™ c·∫£m x√∫c")
        if webrtc_ctx.video_transformer:
            st.info("Camera ƒëang ho·∫°t ƒë·ªông. H∆∞·ªõng khu√¥n m·∫∑t v√†o camera ƒë·ªÉ nh·∫≠n d·∫°ng c·∫£m x√∫c!")
        else:
            st.warning("Camera ch∆∞a ƒë∆∞·ª£c k√≠ch ho·∫°t")

    with st.expander("‚ÑπÔ∏è Th√¥ng tin v·ªÅ c√°c lo·∫°i c·∫£m x√∫c"):
        cols = st.columns(len(EMOTION_CLASSES))
        for i, emotion in enumerate(EMOTION_CLASSES):
            with cols[i]:
                color = EMOTION_COLORS[emotion]
                st.markdown(f"""
                <div style='text-align: center; padding: 10px; border: 2px solid {color}; border-radius: 10px;'>
                    <strong style='color: {color};'>{emotion}</strong>
                </div>
                """, unsafe_allow_html=True)

    st.markdown("---")

if __name__ == "__main__":
    main()