import statistics
import av
import pandas as pd
import streamlit as st
from streamlit_autorefresh import st_autorefresh
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image
import time
from collections import deque
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Nh·∫≠n d·∫°ng c·∫£m x√∫c khu√¥n m·∫∑t",
    page_icon="üòä",
    layout="wide"
)

st_autorefresh(interval=2000, key="emotion_refresh")

# C√°c l·ªõp c·∫£m x√∫c
EMOTION_CLASSES = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']
EMOTION_COLORS = {
    'Angry': '#FF4444',
    'Disgust': '#8B4513',
    'Fear': '#800080',
    'Happy': '#32CD32',
    'Neutral': '#808080',
    'Sad': '#4169E1',
    'Surprise': '#FF8C00'
}


MODEL_CONFIG = {
    "CNN": {"input_size": (48,38), "to_rgb": False, "normalize": True},
    "VGG19": {"input_size": (48,48), "to_rgb": True, "normalize": True}
}

@st.cache_resource
def load_model_by_name(model_name):
    model_files = {
        "CNN": "fer_cnn.h5",
        "VGG19": "fer_vgg19.h5"
    }
    file_path = model_files.get(model_name)
    if not file_path:
        st.error(f"Model '{model_name}' kh√¥ng t·ªìn t·∫°i trong danh s√°ch.")
        return None
    try:
        model = tf.keras.models.load_model(file_path)
        st.success(f"Model '{model_name}' ƒëaÃÉ load thaÃÄnh c√¥ng.")
        return model
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ load model '{model_name}': {e}")
        st.info("Vui l√≤ng ƒë·∫£m b·∫£o file model t·ªìn t·∫°i v√† ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c")
        return None


def preprocess_image(img, model_name):
    config = MODEL_CONFIG[model_name]

    if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):
        if config["to_rgb"]:
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    elif len(img.shape) == 3 and img.shape[2] == 3 and not config["to_rgb"]:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    img = cv2.resize(img, config["input_size"])

    if config["normalize"]:
        img = img.astype('float32') / 255.0

    if model_name == "CNN":
        img = np.expand_dims(img, axis=0)
        img = np.expand_dims(img, axis=-1)
    else:
        img = np.expand_dims(img, axis=0)

    return img


def detect_faces(image):
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    return faces


# def predict_emotion(model, face_image):
#     """D·ª± ƒëo√°n c·∫£m x√∫c t·ª´ ·∫£nh khu√¥n m·∫∑t"""
#     processed_image = preprocess_image(face_image)
#     prediction = model.predict(processed_image, verbose=0)[0]
#     emotion_index = np.argmax(prediction)
#     confidence = np.max(prediction) * 100
#     return EMOTION_CLASSES[emotion_index], confidence

def predict_top_emotions(model, model_name,face_image, top_k=3):
    processed_image = preprocess_image(face_image, model_name)
    prediction = model.predict(processed_image, verbose=0)[0]
    top_indices = prediction.argsort()[-top_k:][::-1]
    top_emotions = [(EMOTION_CLASSES[i], float(prediction[i]) * 100) for i in top_indices]
    return top_emotions


class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.model = load_model_by_name()
        self.latencies = deque(maxlen=200)
        self.fps_values = deque(maxlen=200)
        self.emotions_log = []
        self.last_log_time = time.time()
        self.history = []
        self.device = "cpu"


    def recv(self, frame):
        start_time = time.time()

        img = frame.to_ndarray(format="bgr24")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        frame_emotions = []

        if self.model is not None:
            faces = detect_faces(img_rgb)

            for (x, y, w, h) in faces:
                cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)

                face = img_rgb[y:y + h, x:x + w]

                if face.size > 0:
                    try:
                        top_emotions = predict_top_emotions(self.model, face, top_k=3)
                        frame_emotions.extend([emo for emo, _ in top_emotions])

                        for i, (emo, conf) in enumerate(top_emotions):
                            cv2.putText(img, f"{emo}: {conf:.1f}%",
                                        (x, y - 10 - i * 20),
                                        cv2.FONT_HERSHEY_SIMPLEX,
                                        0.5, (0, 255, 0), 1)

                    except Exception as e:
                        cv2.putText(img, "Error", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,
                                    0.6, (0, 0, 255), 2)

        end_time = time.time()
        latency = end_time - start_time
        fps = 1.0 / latency if latency > 0 else 0

        self.latencies.append(latency)
        self.fps_values.append(fps)
        self.emotions_log.extend(frame_emotions)

        bench_text = f"Latency: {latency * 1000:.1f} ms | FPS: {fps:.1f}"
        cv2.putText(img, bench_text, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        now = time.time()
        if now - self.last_log_time >= 10.0:
            if self.latencies and self.fps_values:
                stats = {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "latency_mean": statistics.mean(self.latencies),
                    "latency_median": statistics.median(self.latencies),
                    "latency_std": statistics.pstdev(self.latencies),
                    "fps_mean": statistics.mean(self.fps_values),
                    "fps_median": statistics.median(self.fps_values),
                    "fps_std": statistics.pstdev(self.fps_values),
                    "top_emotions": pd.Series(self.emotions_log).value_counts().head(3).to_dict()
                }

                self.history.append(stats)

                self.latencies.clear()
                self.fps_values.clear()
                self.emotions_log.clear()
                self.last_log_time = now

        return av.VideoFrame.from_ndarray(img, format="bgr24")

    def get_benchmark(self):
        if len(self.latencies) == 0:
            return None
        arr = np.array(self.latencies)
        fps_arr = np.array(self.fps_values)
        return {
            "Device": self.device.upper(),
            "Latency (ms)": {
                "mean": arr.mean() * 1000,
                "median": np.median(arr) * 1000,
                "std": arr.std() * 1000
            },
            "FPS": {
                "mean": fps_arr.mean(),
                "median": np.median(fps_arr),
                "std": fps_arr.std()
            }
        }

    def get_history_df(self):
        if self.history:
            return pd.DataFrame(self.history)
        else:
            return pd.DataFrame()


def main():
    st.title("üé≠ ·ª®ng d·ª•ng nh·∫≠n d·∫°ng c·∫£m x√∫c khu√¥n m·∫∑t")
    st.markdown("---")


    # Sidebar cho c√°c t√πy ch·ªçn
    st.sidebar.title("T√πy ch·ªçn")
    option = st.sidebar.selectbox(
        "Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠n d·∫°ng:",
        ["üì§ Upload ·∫£nh", "üì∑ S·ª≠ d·ª•ng Camera"]
    )

    if option == "üì§ Upload ·∫£nh":
        st.header("Upload ·∫£nh ƒë·ªÉ nh·∫≠n d·∫°ng c·∫£m x√∫c")

        model_option = st.selectbox(
            "Ch·ªçn m√¥ h√¨nh ƒë·ªÉ nh·∫≠n d·∫°ng:",
            ["CNN", "VGG19"]
        )

        model = load_model_by_name(model_option)

        uploaded_file = st.file_uploader(
            "Ch·ªçn ·∫£nh...",
            type=['png', 'jpg', 'jpeg'],
            help="H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: PNG, JPG, JPEG"
        )

        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            image_array = np.array(image)

            col1, col2 = st.columns(2)

            with col1:
                st.subheader("·∫¢nh g·ªëc")
                st.image(image, use_container_width=True)

            with col2:
                st.subheader("K·∫øt qu·∫£ nh·∫≠n d·∫°ng")
                faces = detect_faces(image_array)
                if len(faces) > 0:
                    result_image = image_array.copy()
                    emotions_detected = []
                    for i, (x, y, w, h) in enumerate(faces):
                        cv2.rectangle(result_image, (x, y), (x + w, y + h), (255, 0, 0), 2)
                        face = image_array[y:y + h, x:x + w]
                        top_emotions = predict_top_emotions(model, model_option,face, top_k=3)
                        emotions_detected.append(top_emotions)  # l∆∞u c·∫£ top 3

                        best_emotion, best_conf = top_emotions[0]
                        cv2.putText(result_image, f"{best_emotion}: {best_conf:.1f}%",
                                    (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 5)

                    st.image(result_image, use_container_width=True)

                    st.subheader("Chi ti·∫øt k·∫øt qu·∫£:")
                    for i, top_emotions in enumerate(emotions_detected):
                        st.markdown(f"<strong>Khu√¥n m·∫∑t {i + 1}:</strong>", unsafe_allow_html=True)
                        for emo, conf in top_emotions:
                            color = EMOTION_COLORS.get(emo, '#000000')
                            st.markdown(f"""
                            <div style='padding: 5px; border-left: 4px solid {color}; margin: 3px 0;'>
                                {emo}: {conf:.1f}%
                            </div>
                            """, unsafe_allow_html=True)
                else:
                    st.warning("Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t trong ·∫£nh!")
                    st.info("üí° M·∫πo: H√£y th·ª≠ v·ªõi ·∫£nh c√≥ khu√¥n m·∫∑t r√µ r√†ng v√† kh√¥ng b·ªã che khu·∫•t")

    elif option == "üì∑ S·ª≠ d·ª•ng Camera":
        st.header("S·ª≠ d·ª•ng Camera ƒë·ªÉ nh·∫≠n d·∫°ng c·∫£m x√∫c real-time")

        st.info("üìå Cho ph√©p truy c·∫≠p camera khi ƒë∆∞·ª£c y√™u c·∫ßu")
        rtc_configuration = RTCConfiguration({
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        })

        webrtc_ctx = webrtc_streamer(
            key="emotion-detection",
            video_processor_factory=VideoTransformer,
            rtc_configuration=rtc_configuration,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

        stats_placeholder = st.empty()
        table_placeholder = st.empty()

        if webrtc_ctx.video_processor:
            df = webrtc_ctx.video_processor.get_history_df()
            bench = webrtc_ctx.video_processor.get_benchmark()

            if not df.empty:
                st.subheader("üìä Th·ªëng k√™ Benchmark & C·∫£m x√∫c")
                st.dataframe(df)

            if bench:
                st.markdown("### ‚ö° Benchmark (Realtime)")
                st.write(f"**Device**: {bench['Device']}")
                st.write(f"**Latency (ms)** ‚Üí Mean: {bench['Latency (ms)']['mean']:.1f}, "
                         f"Median: {bench['Latency (ms)']['median']:.1f}, "
                         f"Std: {bench['Latency (ms)']['std']:.1f}")
                st.write(f"**FPS** ‚Üí Mean: {bench['FPS']['mean']:.1f}, "
                         f"Median: {bench['FPS']['median']:.1f}, "
                         f"Std: {bench['FPS']['std']:.1f}")

    with st.expander("‚ÑπÔ∏è Th√¥ng tin v·ªÅ c√°c lo·∫°i c·∫£m x√∫c"):
        cols = st.columns(len(EMOTION_CLASSES))
        for i, emotion in enumerate(EMOTION_CLASSES):
            with cols[i]:
                color = EMOTION_COLORS[emotion]
                st.markdown(f"""
                <div style='text-align: center; padding: 10px; border: 2px solid {color}; border-radius: 10px;'>
                    <strong style='color: {color};'>{emotion}</strong>
                </div>
                """, unsafe_allow_html=True)

    st.markdown("---")

if __name__ == "__main__":
    main()